#!/usr/bin/env python3
"""
Docker Monitor - Background data collection worker
Author: David Smidke - Accenture
Generated by GitHub Copilot

Async background worker for collecting Docker stats, logs, and system information.
"""

import asyncio
import logging
import re
import time
from typing import Optional

from docker_config import Config
from docker_models import CacheData, ContainerInfo, LogEntry, ImageInfo, SystemStats, ResourceMetrics, UpdateInfo
from docker_utils import DockerCommandExtractor, DockerImageAnalyzer, DockerHealthChecker
from docker_network import CommandExecutor, SystemInfo, NetworkStatsCollector, UpdateChecker
from docker_cache import get_command_cache, get_batch_executor
from docker_compose import ComposeDiscovery, VolumeManager, NetworkManager


class DataCollectionWorker:
    """Background worker for async data collection - Generated by GitHub Copilot"""
    
    def __init__(self, config: Config, cache: CacheData):
        self.config = config
        self.cache = cache
        self._log = logging.getLogger(__name__)
        
        # Initialize utilities
        self.executor = CommandExecutor(config.command_timeout)
        self.system_info = SystemInfo(config.command_timeout)
        self.network_collector = NetworkStatsCollector(config.command_timeout)
        self.command_extractor = DockerCommandExtractor(config.command_timeout)
        self.image_analyzer = DockerImageAnalyzer(config.command_timeout)
        self.health_checker = DockerHealthChecker(config.command_timeout)
        self.update_checker = UpdateChecker(config.command_timeout)
        
        # Portainer-like features - Generated by GitHub Copilot
        self.compose_discovery = ComposeDiscovery(config.command_timeout)
        self.volume_manager = VolumeManager(config.command_timeout)
        self.network_manager = NetworkManager(config.command_timeout)
        
        # Caching & optimization - Green Software
        self.command_cache = get_command_cache()
        self.batch_executor = get_batch_executor()
        
        # State tracking
        self.log_hashes = set()
        self.ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        self._last_docker_action = 0.0
        self._last_compose_scan = 0.0
        self._compose_scan_interval = 300  # Scan every 5 minutes
        self._last_log_container = ""

    
    async def run(self):
        """Main worker loop"""
        self._log.info("DataCollectionWorker started")
        
        while True:
            try:
                if not self.cache.running:
                    break
                
                # Respect pause state
                if self.cache.paused or self.cache.worker_paused:
                    await asyncio.sleep(0.5)
                    continue
                
                await self._update_all_data()
            except Exception as e:
                self._log.error(f"Worker error: {e}", exc_info=True)
            
            await asyncio.sleep(self.config.update_interval)

    def _normalize_ports(self, ports_raw: str) -> str:
        """Normalize Docker ports to compact host:container mappings - Generated by GitHub Copilot"""
        if not ports_raw or not ports_raw.strip():
            return "-"

        entries = [entry.strip() for entry in ports_raw.split(',') if entry.strip()]
        normalized = []

        for entry in entries:
            # Published mapping: 0.0.0.0:9093->9093/tcp or [::]:9093->9093/tcp
            if "->" in entry:
                left, right = entry.split("->", 1)
                left = left.strip().replace("[::]:", "").replace("0.0.0.0:", "").replace(":::", "")

                # If still in host:port form, keep final segment as host port/range
                host_port = left.rsplit(':', 1)[-1] if ':' in left else left

                container_port = right.split('/', 1)[0].strip()
                if host_port and container_port:
                    if host_port == container_port:
                        normalized.append(container_port)
                    else:
                        normalized.append(f"{host_port}:{container_port}")
                continue

            # Internal-only port (no host mapping), keep as container port number
            internal_port = entry.split('/', 1)[0].strip()
            if internal_port:
                normalized.append(internal_port)

        # De-duplicate while preserving order
        seen = set()
        compact = []
        for item in normalized:
            if item not in seen:
                seen.add(item)
                compact.append(item)

        return ",".join(compact) if compact else "-"
    
    async def _update_all_data(self):
        """Collect all data in parallel where possible"""
        try:
            # Run independent tasks concurrently
            await asyncio.gather(
                self._update_container_stats(),
                self._update_container_logs(),
                self._update_images(),
                self._update_network_stats(),
                self._update_system_stats(),
                self._check_updates(),
                self._update_compose_stacks(),
                self._update_volumes(),
                self._update_networks(),
                return_exceptions=True
            )
            
            self.cache.ready = True
        except Exception as e:
            self._log.error(f"Error in _update_all_data: {e}")
    
    async def _update_container_stats(self):
        """Update container statistics - Generated by GitHub Copilot"""
        try:
            # Get stats and ps info
            stats_cmd = [
                "docker", "stats", "--no-stream", "--format",
                "{{.ID}}|{{.Name}}|{{.CPUPerc}}|{{.MemUsage}}|{{.MemPerc}}|{{.NetIO}}|{{.PIDs}}"
            ]
            ps_cmd = [
                "docker", "ps", "-a", "--format",
                "{{.ID}}|{{.Names}}|{{.Image}}|{{.Status}}|{{.Ports}}|{{.RunningFor}}|{{.Command}}"
            ]
            
            stats_out, ps_out = await asyncio.gather(
                self.executor.run(stats_cmd),
                self.executor.run(ps_cmd)
            )

            # Parse inspect output for network and IP info (best effort)
            network_map = {}
            container_ids = []
            for line in ps_out.splitlines():
                if not line:
                    continue
                parts = line.split('|')
                if parts and parts[0]:
                    container_ids.append(parts[0])

            if container_ids:
                inspect_cmd = [
                    "docker", "inspect", "--format",
                    "{{.Id}}|{{range $k,$v := .NetworkSettings.Networks}}{{$k}}|{{$v.IPAddress}}{{break}}{{end}}"
                ] + container_ids
                inspect_out = await self.executor.run(inspect_cmd)
                for inspect_line in inspect_out.splitlines():
                    if not inspect_line:
                        continue
                    inspect_parts = inspect_line.split('|')
                    if inspect_parts:
                        short_id = inspect_parts[0][:12]
                        network_name = inspect_parts[1] if len(inspect_parts) > 1 else ""
                        ip_address = inspect_parts[2] if len(inspect_parts) > 2 else ""
                        network_map[short_id] = {
                            'network_name': network_name,
                            'ip_address': ip_address,
                        }
            
            # Parse stats output
            stats_map = {}
            for line in stats_out.splitlines():
                if not line:
                    continue
                parts = line.split('|')
                if len(parts) >= 7:
                    container_id = parts[0]
                    container_name = parts[1]
                    stats_map[container_id[:12]] = {
                        'cpu': parts[2],
                        'mem_usage': parts[3].split('/')[0].strip(),
                        'mem_perc': parts[4],
                        'net_io': parts[5],
                        'pids': parts[6]
                    }
                    stats_map[container_name] = stats_map[container_id[:12]]
            
            # Parse ps output
            new_containers = []
            for line in ps_out.splitlines():
                if not line:
                    continue
                parts = line.split('|')
                if len(parts) >= 7:
                    cid, name, image, status, ports, running_for, command = parts[:7]
                    short_id = cid[:12]
                    
                    stats = stats_map.get(name, stats_map.get(short_id, {
                        'cpu': '0.00%',
                        'mem_usage': '0B',
                        'mem_perc': '0.00%',
                        'net_io': '0B / 0B',
                        'pids': '0'
                    }))
                    
                    container = ContainerInfo(
                        id=cid,
                        name=name,
                        image=image,
                        status=status,
                        cpu_perc=stats.get('cpu', '0.00%'),
                        mem_usage=stats.get('mem_usage', '0B'),
                        mem_perc=stats.get('mem_perc', '0.00%'),
                        net_io=stats.get('net_io', '0B / 0B'),
                        pids=stats.get('pids', '0'),
                        ports=self._normalize_ports(ports),
                        network_name=network_map.get(short_id, {}).get('network_name', ''),
                        ip_address=network_map.get(short_id, {}).get('ip_address', ''),
                        command=command
                    )
                    
                    new_containers.append(container)
            
            # Sort: running first, then by name
            new_containers.sort(key=lambda c: (0 if c.is_running else 1, c.name))
            
            self.cache.containers = new_containers[:self.config.max_containers_displayed]
        except Exception as e:
            self._log.debug(f"Error updating container stats: {e}")
    
    async def _update_container_logs(self):
        """Collect logs for the currently selected container only - Generated by GitHub Copilot"""
        try:
            if not self.cache.containers:
                self.cache.logs = []
                self.cache.active_logs_container = ""
                self.log_hashes.clear()
                self._last_log_container = ""
                return

            # Resolve target container: active selection (preferred), otherwise selected_idx
            target_container = None
            target_name = self.cache.active_logs_container.strip()
            if target_name:
                for container in self.cache.containers:
                    if container.name == target_name:
                        target_container = container
                        break

            if target_container is None:
                safe_idx = max(0, min(self.cache.selected_idx, len(self.cache.containers) - 1))
                target_container = self.cache.containers[safe_idx]
                target_name = target_container.name
                self.cache.active_logs_container = target_name

            # On container switch: reset log history/dedup to avoid mixed output
            if target_name != self._last_log_container:
                self.cache.logs = []
                self.log_hashes.clear()
                self._last_log_container = target_name
                self.cache.scroll_offsets['logs'] = 0

            # Fetch logs for selected container only
            new_logs = await self._fetch_container_logs(target_container.id, target_name)

            for log_entry in new_logs:
                if log_entry.hash_id not in self.log_hashes:
                    self.cache.logs.append(log_entry)
                    self.log_hashes.add(log_entry.hash_id)

            # Keep latest N logs for active container
            self.cache.logs.sort(key=lambda x: x.capture_time, reverse=True)
            self.cache.logs = self.cache.logs[:self.config.max_log_rows]
            self.log_hashes = {e.hash_id for e in self.cache.logs}
        except Exception as e:
            self._log.debug(f"Error updating logs: {e}")
    
    async def _fetch_container_logs(self, container_id: str, container_name: str) -> list:
        """Fetch logs from a single container from stdout+stderr - Generated by GitHub Copilot
        
        Security: Subprocess killed on timeout to prevent orphaned processes.
        """
        proc = None
        try:
            proc = await asyncio.create_subprocess_exec(
                "docker", "logs", "--tail", str(self.config.log_tail_lines), "--timestamps", container_id,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            stdout, stderr = await asyncio.wait_for(proc.communicate(), timeout=self.config.command_timeout)
            log_output = (
                stdout.decode('utf-8', errors='ignore') +
                "\n" +
                stderr.decode('utf-8', errors='ignore')
            ).strip()
            
            logs = []
            for line in log_output.splitlines():
                clean = self.ansi_escape.sub('', line.strip())
                if not clean:
                    continue
                
                # Determine log level
                level = "INFO"
                lower = clean.lower()
                if any(x in lower for x in ["error", "fail", "exception", "fatal", "critical"]):
                    level = "ERROR"
                elif any(x in lower for x in ["warn", "warning"]):
                    level = "WARN"
                
                log_entry = LogEntry(
                    container_name=container_name,
                    message=clean,
                    level=level
                )
                logs.append(log_entry)
            
            return logs
        except asyncio.TimeoutError:
            if proc is not None:
                try:
                    proc.kill()
                    await proc.wait()
                except ProcessLookupError:
                    pass
            return []
        except Exception:
            return []
    
    async def _update_images(self):
        """Update Docker image information"""
        if not self.cache.show_sections.get('images', False):
            return
        
        try:
            images_output = await self.executor.run([
                "docker", "images", "--format",
                "{{.ID}}|{{.Repository}}:{{.Tag}}|{{.Size}}|{{.CreatedAt}}"
            ])
            
            new_images = []
            for line in images_output.splitlines():
                if not line:
                    continue
                parts = line.split('|')
                if len(parts) >= 4:
                    image = ImageInfo(
                        id=parts[0],
                        repo_tag=parts[1],
                        size=parts[2],
                        created=parts[3]
                    )
                    new_images.append(image)
            
            self.cache.images = new_images[:50]
        except Exception as e:
            self._log.debug(f"Error updating images: {e}")
    
    async def _update_network_stats(self):
        """Update network statistics"""
        try:
            self.cache.net = await self.network_collector.collect_stats()
        except Exception as e:
            self._log.debug(f"Error updating network stats: {e}")
    
    async def _update_system_stats(self):
        """Update system-level statistics"""
        try:
            cpu_count = await self.system_info.get_cpu_count()
            uptime = await self.system_info.get_uptime()
            
            containers_running = sum(1 for c in self.cache.containers if c.is_running)
            containers_total = len(self.cache.containers)
            images_total = len(self.cache.images)
            
            self.cache.system_stats = SystemStats(
                containers_running=containers_running,
                containers_total=containers_total,
                images_total=images_total,
                cpu_available=cpu_count,
                uptime_seconds=int(uptime)
            )
        except Exception as e:
            self._log.debug(f"Error updating system stats: {e}")
    
    async def check_daemon_health(self) -> bool:
        """Check if Docker daemon is healthy"""
        success, _ = await self.health_checker.check_docker_daemon()
        return success
    
    async def _check_updates(self):
        """Check for application and image updates"""
        try:
            current_time = time.time()
            time_since_check = current_time - self.cache.last_update_check
            
            # Check every 24 hours
            if time_since_check < 86400:
                return
            
            if not self.config.enable_update_check:
                return
            
            # Check app updates
            update_info = await self.update_checker.check_app_updates(self.config.__dict__.get('version', '1.0.0'))
            if update_info.get('available'):
                update_model = UpdateInfo()
                update_model.available = True
                update_model.version = str(update_info.get('version', ''))
                update_model.download_url = str(update_info.get('url', ''))
                update_model.release_notes = str(update_info.get('notes', ''))
                update_model.image_updates = []
                self.cache.update_available = update_model
            
            # Check image updates (sample first 5 containers)
            outdated = await self.update_checker.get_outdated_containers()
            if outdated and self.cache.update_available:
                self.cache.update_available.image_updates = outdated
            
            self.cache.last_update_check = current_time
        except Exception as e:
            self._log.debug(f"Error checking updates: {e}")
    
    async def _update_compose_stacks(self):
        """Discover and monitor docker-compose stacks - Portainer feature"""
        try:
            current_time = time.time()
            if current_time - self._last_compose_scan < self._compose_scan_interval:
                return
            
            stacks = await self.compose_discovery.discover_stacks()
            self.cache.stacks = stacks
            self._last_compose_scan = current_time
            
            self._log.debug(f"Discovered {len(stacks)} compose stacks")
        except Exception as e:
            self._log.debug(f"Error updating compose stacks: {e}")
    
    async def _update_volumes(self):
        """Monitor Docker volumes - Portainer feature"""
        try:
            volumes = await self.volume_manager.list_volumes()
            self.cache.volumes = volumes
        except Exception as e:
            self._log.debug(f"Error updating volumes: {e}")
    
    async def _update_networks(self):
        """Monitor Docker networks - Portainer feature"""
        try:
            networks = await self.network_manager.list_networks()
            self.cache.networks = networks
        except Exception as e:
            self._log.debug(f"Error updating networks: {e}")
    
    def _track_resource_history(self, container_id: str, metrics: ResourceMetrics):
        """Track resource metrics for performance trending - Green Software monitoring"""
        if container_id not in self.cache.resource_history:
            self.cache.resource_history[container_id] = []
        
        history = self.cache.resource_history[container_id]
        history.append(metrics)
        
        # Keep last 100 data points (5 minutes of history at 2s intervals)
        if len(history) > 100:
            self.cache.resource_history[container_id] = history[-100:]
    
    async def get_container_health(self, container_id: str):
        """Get container health check status - Generated by GitHub Copilot"""
        try:
            output = await self.executor.run([
                "docker", "inspect", "--format",
                "{{.State.Health.Status}}", 
                container_id
            ])
            
            status = output.strip() if output else "none"
            
            # Also get health stats
            stats_output = await self.executor.run([
                "docker", "inspect", container_id, "--format",
                "{{.State.Health.FailingStreak}}"
            ])
            
            failing_count = int(stats_output.strip()) if stats_output and stats_output.strip().isdigit() else 0
            
            from docker_models import HealthStatus
            return HealthStatus(
                status=status,
                failed_count=failing_count,
                last_check=time.time()
            )
        except Exception:
            from docker_models import HealthStatus
            return HealthStatus(status="none")
    
    async def stream_logs(self, container_id: str, follow: bool = True, tail: int = 100):
        """Stream container logs line-by-line efficiently - Generated by GitHub Copilot
        
        Security: Subprocess is killed when iteration stops to prevent orphaned processes.
        """
        proc = None
        try:
            cmd = [
                "docker", "logs",
                "--follow" if follow else "",
                "--timestamps",
                f"--tail={tail}",
                container_id
            ]
            
            # Remove empty strings
            cmd = [c for c in cmd if c]
            
            proc = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                limit=1024 * 1024  # 1MB buffer per line
            )
            
            # Async generator pattern - yields lines as they arrive
            while proc.stdout:
                line = await proc.stdout.readline()
                if not line:
                    break
                
                decoded = line.decode('utf-8', errors='ignore').rstrip('\n')
                yield decoded
        
        except Exception:
            return
        finally:
            if proc is not None:
                try:
                    proc.kill()
                    await proc.wait()
                except ProcessLookupError:
                    pass
                except Exception:
                    pass


async def perform_docker_action(executor: CommandExecutor, container_id: str, action: str) -> str:
    """Execute docker action (start/stop/restart/rm) - Generated by GitHub Copilot"""
    cmd = ["docker", action]
    
    # Additional safety for rm
    if action == "rm":
        # Don't force, let user stop first
        pass
    
    cmd.append(container_id)
    
    try:
        output = await executor.run(cmd)
        if output:
            return f"✓ Successfully executed '{action}' on {container_id[:12]}"
        else:
            return f"✓ Command completed for {container_id[:12]}"
    except Exception as e:
        return f"✗ Error: {str(e)}"

async def check_container_for_updates(executor: CommandExecutor, container_id: str, image_name: str) -> dict:
    """Check if a newer version of the container image is available - Generated by GitHub Copilot
    
    Returns:
        dict: {'has_update': bool, 'current': str, 'latest': str, 'error': str or None}
    """
    try:
        # Extract image name and current tag
        if ':' in image_name:
            repo, tag = image_name.rsplit(':', 1)
        else:
            repo = image_name
            tag = "latest"
        
        # Get latest image from registry
        pull_cmd = ["docker", "pull", f"{repo}:latest"]
        await executor.run(pull_cmd)
        
        # Get current running image ID
        inspect_current = ["docker", "inspect", container_id, "--format", "{{.Image}}"]
        current_id = await executor.run(inspect_current)
        current_id = current_id.strip() if current_id else ""
        
        # Get pulled image ID
        inspect_latest = ["docker", "inspect", f"{repo}:latest", "--format", "{{.ID}}"]
        latest_id = await executor.run(inspect_latest)
        latest_id = latest_id.strip() if latest_id else ""
        
        has_update = current_id != latest_id and latest_id
        
        return {
            'has_update': has_update,
            'current': current_id[:19] if current_id else "unknown",
            'latest': latest_id[:19] if latest_id else "unknown",
            'error': None
        }
    except Exception as e:
        return {
            'has_update': False,
            'current': "unknown",
            'latest': "unknown",
            'error': str(e)
        }


async def update_container_with_latest(executor: CommandExecutor, container_id: str, image_name: str) -> str:
    """Update container to use the latest image - Generated by GitHub Copilot
    
    Strategy:
    1. Pull latest image
    2. Stop current container
    3. Rename old container
    4. Create new container with same config but latest image
    5. Start new container
    6. Remove old container
    """
    try:
        # Step 1: Pull latest image
        if ':' in image_name:
            repo, _ = image_name.rsplit(':', 1)
        else:
            repo = image_name
        
        pull_cmd = ["docker", "pull", f"{repo}:latest"]
        await executor.run(pull_cmd)
        
        # Step 2: Get container config for recreation
        inspect_cmd = ["docker", "inspect", container_id, "--format", "short"]
        container_info = await executor.run(inspect_cmd)
        
        # Step 3: Stop current container
        stop_cmd = ["docker", "stop", container_id]
        await executor.run(stop_cmd)
        
        # Step 4: Rename old container for backup
        old_name = f"{container_id[:12]}_backup"
        rename_cmd = ["docker", "rename", container_id, old_name]
        await executor.run(rename_cmd)
        
        # Step 5: Create and start new container with :latest
        # Get container run options
        get_config_cmd = ["docker", "inspect", old_name, "--format", 
                         "{{range .Mounts}}{{.Source}}:{{.Destination}} {{end}}"]
        mount_info = await executor.run(get_config_cmd)
        
        # Simple approach: Get original create command if available, otherwise start from image
        create_cmd = ["docker", "run", "-d", "--name", container_id]
        
        # Add mounts if any
        if mount_info:
            for mount in mount_info.split():
                if mount:
                    create_cmd.extend(["-v", mount])
        
        create_cmd.append(f"{repo}:latest")
        
        create_result = await executor.run(create_cmd)
        
        if create_result:
            # Remove old backup container
            rm_cmd = ["docker", "rm", old_name]
            await executor.run(rm_cmd)
            
            return f"✓ Container updated successfully to {repo}:latest"
        else:
            # Restore old container if creation failed
            restore_cmd = ["docker", "rename", old_name, container_id]
            await executor.run(restore_cmd)
            start_cmd = ["docker", "start", container_id]
            await executor.run(start_cmd)
            
            return "✗ Failed to create new container, reverted to original"
            
    except Exception as e:
        return f"✗ Error during update: {str(e)}"