#!/usr/bin/env python3
"""
Docker Monitor - Docker Compose Integration
Author: David Smidke - Accenture
Generated by GitHub Copilot

Docker Compose stack discovery, management, and visualization with Portainer-like features.
Includes project detection, service grouping, and multi-compose file support.
"""

import asyncio
import json
import logging
import os
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any


@dataclass
class ComposeService:
    """Individual service within a compose stack"""
    name: str
    image: str
    container_id: str = ""
    status: str = "unknown"
    ports: List[str] = field(default_factory=list)
    volumes: List[str] = field(default_factory=list)
    environment: Dict[str, str] = field(default_factory=dict)
    depends_on: List[str] = field(default_factory=list)
    networks: List[str] = field(default_factory=list)
    labels: Dict[str, str] = field(default_factory=dict)
    cpu_limit: str = ""
    memory_limit: str = ""
    
    @property
    def is_running(self) -> bool:
        return self.status.lower() in ("running", "up")


@dataclass
class ComposeStack:
    """Docker Compose stack (project) - Generated by GitHub Copilot"""
    name: str
    file_path: str
    services: Dict[str, ComposeService] = field(default_factory=dict)
    version: str = ""
    status: str = "unknown"  # running, stopped, partial
    container_count: int = 0
    running_count: int = 0
    created_at: float = 0.0
    
    @property
    def is_healthy(self) -> bool:
        """Stack is healthy if all services are running"""
        if not self.services:
            return False
        return all(svc.is_running for svc in self.services.values())
    
    @property
    def health_percentage(self) -> int:
        """Percentage of services running"""
        if not self.services:
            return 0
        running = sum(1 for svc in self.services.values() if svc.is_running)
        return int((running / len(self.services)) * 100)


@dataclass
class DockerVolume:
    """Docker volume information - Portainer feature"""
    name: str
    driver: str = "local"
    mountpoint: str = ""
    labels: Dict[str, str] = field(default_factory=dict)
    size_mb: float = 0.0
    used_containers: List[str] = field(default_factory=list)


@dataclass
class DockerNetwork:
    """Docker network information - Portainer feature"""
    id: str
    name: str
    driver: str = "bridge"
    scope: str = "local"
    containers: List[str] = field(default_factory=list)
    labels: Dict[str, str] = field(default_factory=dict)
    subnet: str = ""


class ComposeDiscovery:
    """Discovery and parsing of docker-compose files - Generated by GitHub Copilot"""
    
    def __init__(self, timeout: int = 10):
        self._timeout = timeout
        self._log = logging.getLogger(__name__)
        self._compose_cache: Dict[str, ComposeStack] = {}
        self._cache_time = {}
    
    async def discover_stacks(self, search_paths: Optional[List[str]] = None) -> Dict[str, ComposeStack]:
        """
        Discover docker-compose projects across the system
        
        Security: Respects .gitignore and common exclusion patterns
        Green Software: Caches results to avoid repeated filesystem scans
        """
        if search_paths is None:
            search_paths = [str(Path.home()), "/opt", "/srv", "/home"]
        
        stacks = {}
        
        # First, get compose projects from docker itself
        stacks.update(await self._get_docker_projects())
        
        # Then, scan filesystem for compose files
        for search_path in search_paths:
            if not os.path.exists(search_path):
                continue
            scanned = await self._scan_for_compose_files(search_path)
            stacks.update(scanned)
        
        self._compose_cache = stacks
        return stacks
    
    async def _get_docker_projects(self) -> Dict[str, ComposeStack]:
        """Get compose projects from 'docker compose ls' output"""
        stacks = {}
        try:
            import subprocess
            result = subprocess.run(
                ["docker", "compose", "ls", "--format", "json"],
                capture_output=True,
                timeout=self._timeout,
                text=True
            )
            if result.returncode == 0 and result.stdout:
                projects = json.loads(result.stdout)
                for proj in projects:
                    stack = ComposeStack(
                        name=proj.get("Name", "unknown"),
                        file_path=proj.get("ConfigFiles", ""),
                        status="running" if proj.get("Status", "").lower() == "running" else "stopped",
                        container_count=int(proj.get("Containers", 0)) if proj.get("Containers") else 0
                    )
                    stacks[stack.name] = stack
        except Exception as e:
            self._log.debug(f"Error getting Docker projects: {e}")
        
        return stacks
    
    async def _scan_for_compose_files(self, root_path: str, max_depth: int = 3) -> Dict[str, ComposeStack]:
        """Recursively scan for docker-compose.yml files"""
        stacks = {}
        exclude_patterns = ['.git', '__pycache__', 'node_modules', '.venv', 'venv']
        
        try:
            for root, dirs, files in os.walk(root_path, topdown=True):
                # Prune excluded directories
                dirs[:] = [d for d in dirs if d not in exclude_patterns]
                
                # Check depth (green software: limit filesystem traversal)
                depth = root[len(root_path):].count(os.sep)
                if depth > max_depth:
                    dirs.clear()
                    continue
                
                # Look for compose files
                for compose_file in ['docker-compose.yml', 'docker-compose.yaml', 'compose.yml', 'compose.yaml']:
                    file_path = Path(root) / compose_file
                    if file_path.exists():
                        stack = await self._parse_compose_file(str(file_path))
                        if stack:
                            stacks[stack.name] = stack
        except Exception as e:
            self._log.debug(f"Error scanning {root_path}: {e}")
        
        return stacks
    
    async def _parse_compose_file(self, file_path: str) -> Optional[ComposeStack]:
        """Parse a docker-compose.yml file"""
        try:
            # Try using pyyaml first
            try:
                yaml = __import__("yaml")
                with open(file_path, 'r') as f:
                    data = yaml.safe_load(f)
            except ImportError:
                # Fallback: simple json parsing (many projects use JSON-style output)
                import subprocess
                result = subprocess.run(
                    ["docker", "compose", "-f", file_path, "config"],
                    capture_output=True,
                    timeout=self._timeout,
                    text=True
                )
                if result.returncode != 0:
                    return None
                data = json.loads(result.stdout)
            
            if not data or 'services' not in data:
                return None
            
            project_name = Path(file_path).parent.name
            stack = ComposeStack(
                name=project_name,
                file_path=file_path,
                version=str(data.get('version', 'unknown'))
            )
            
            # Parse services
            for service_name, service_config in data.get('services', {}).items():
                service = ComposeService(
                    name=service_name,
                    image=service_config.get('image', ''),
                    ports=service_config.get('ports', []),
                    volumes=service_config.get('volumes', []),
                    environment=service_config.get('environment', {}),
                    depends_on=service_config.get('depends_on', []),
                    networks=service_config.get('networks', []),
                    labels=service_config.get('labels', {}),
                    cpu_limit=service_config.get('cpus', ''),
                    memory_limit=service_config.get('mem_limit', '')
                )
                stack.services[service_name] = service
            
            return stack
        except Exception as e:
            self._log.debug(f"Error parsing {file_path}: {e}")
            return None
    
    async def get_stack_details(self, stack_name: str) -> Optional[ComposeStack]:
        """Get detailed information about a specific stack"""
        if stack_name in self._compose_cache:
            return self._compose_cache[stack_name]
        
        # Try to load from docker
        try:
            import subprocess
            result = subprocess.run(
                ["docker", "compose", "-p", stack_name, "ps", "--format", "json"],
                capture_output=True,
                timeout=self._timeout,
                text=True
            )
            if result.returncode == 0 and result.stdout:
                containers = json.loads(result.stdout)
                # Merge with cached data
                if stack_name in self._compose_cache:
                    stack = self._compose_cache[stack_name]
                    for container in containers:
                        service_name = container.get('Service', '')
                        if service_name in stack.services:
                            stack.services[service_name].container_id = container.get('ID', '')
                            stack.services[service_name].status = container.get('State', 'unknown')
                    return stack
        except Exception as e:
            self._log.debug(f"Error getting stack details: {e}")
        
        return self._compose_cache.get(stack_name)


class VolumeManager:
    """Manage and monitor Docker volumes - Portainer feature"""
    
    def __init__(self, timeout: int = 5):
        self._timeout = timeout
        self._log = logging.getLogger(__name__)
        self._volume_cache: Dict[str, DockerVolume] = {}
    
    async def list_volumes(self) -> Dict[str, DockerVolume]:
        """List all Docker volumes"""
        self._volume_cache.clear()
        try:
            import subprocess
            result = subprocess.run(
                ["docker", "volume", "ls", "--format", "json"],
                capture_output=True,
                timeout=self._timeout,
                text=True
            )
            if result.returncode == 0 and result.stdout:
                volumes = json.loads(result.stdout)
                for vol in volumes:
                    volume = DockerVolume(
                        name=vol.get('Name', ''),
                        driver=vol.get('Driver', 'local'),
                        labels=vol.get('Labels', {})
                    )
                    self._volume_cache[volume.name] = volume
        except Exception as e:
            self._log.debug(f"Error listing volumes: {e}")
        
        return self._volume_cache
    
    async def get_volume_usage(self, volume_name: str) -> float:
        """Get volume usage in MB"""
        try:
            import subprocess
            import os
            result = subprocess.run(
                ["docker", "volume", "inspect", volume_name],
                capture_output=True,
                timeout=self._timeout,
                text=True
            )
            if result.returncode == 0:
                data = json.loads(result.stdout)[0]
                mountpoint = data.get('Mountpoint', '')
                if mountpoint and os.path.exists(mountpoint):
                    total_size = sum(
                        os.path.getsize(os.path.join(path, filename))
                        for path, dirs, files in os.walk(mountpoint)
                        for filename in files
                    )
                    return total_size / (1024 * 1024)  # Convert to MB
        except Exception as e:
            self._log.debug(f"Error measuring volume: {e}")
        
        return 0.0


class NetworkManager:
    """Manage and visualize Docker networks - Portainer feature"""
    
    def __init__(self, timeout: int = 5):
        self._timeout = timeout
        self._log = logging.getLogger(__name__)
        self._network_cache: Dict[str, DockerNetwork] = {}
    
    async def list_networks(self) -> Dict[str, DockerNetwork]:
        """List all Docker networks"""
        self._network_cache.clear()
        try:
            import subprocess
            result = subprocess.run(
                ["docker", "network", "ls", "--format", "json"],
                capture_output=True,
                timeout=self._timeout,
                text=True
            )
            if result.returncode == 0 and result.stdout:
                networks = json.loads(result.stdout)
                for net in networks:
                    network = DockerNetwork(
                        id=net.get('ID', ''),
                        name=net.get('Name', ''),
                        driver=net.get('Driver', ''),
                        scope=net.get('Scope', 'local')
                    )
                    self._network_cache[network.name] = network
        except Exception as e:
            self._log.debug(f"Error listing networks: {e}")
        
        return self._network_cache
    
    async def get_network_containers(self, network_name: str) -> List[str]:
        """Get containers attached to a network"""
        try:
            import subprocess
            result = subprocess.run(
                ["docker", "network", "inspect", network_name],
                capture_output=True,
                timeout=self._timeout,
                text=True
            )
            if result.returncode == 0:
                data = json.loads(result.stdout)[0]
                containers = data.get('Containers', {})
                return list(containers.keys())
        except Exception as e:
            self._log.debug(f"Error getting network containers: {e}")
        
        return []
